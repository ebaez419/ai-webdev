{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>AI in Web Development</h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "<center><h2>Lesson 04</h2></center>\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snsie/ai-webdev/blob/main/lessons/lesson-04/lesson-04.ipynb)"
      ],
      "metadata": {
        "id": "IPgoWtoIJQOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on [this example](https://colab.research.google.com/github/ehennis/ReinforcementLearning/blob/master/05-DQN.ipynb#scrollTo=DPWjJiOZ2uVd)\n",
        "\n"
      ],
      "metadata": {
        "id": "KPb9IjzQmKOU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf4_QpE72uVZ"
      },
      "source": [
        "<h1 align=\"center\">Reinforcement Learning (RL)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/snsie/ai-webdev/blob/main/images/what-is-reinforcement-learning.png?raw=true\" width='320px'/></center>\n"
      ],
      "metadata": {
        "id": "VqGHxllEo3h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Environment\n",
        "  * The stage that contains the simulation\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Agent\n",
        "  * The entity making decisions\n",
        "  * Can be represented as a neural network\n",
        "\n",
        "<br/>\n",
        "\n",
        "###States\n",
        "* Set of observations that agents that can be performed by the agent\n",
        "* example: agent's position\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Actions\n",
        "* Set of activities that can be performed by the agent\n",
        "* example: move right, move left\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Rewards\n",
        "* Provides agents feedback about their performance\n"
      ],
      "metadata": {
        "id": "sdedL4816Qp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>Cart Pole Example</h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "[Gym Docs](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
      ],
      "metadata": {
        "id": "P12W_YPa09UE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Bad Cart          |  Good Cart  |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "| <img src='https://github.com/snsie/ai-webdev/blob/main/images/cartpole-initial.gif?raw=true' width=\"300\"/>  | <img src='https://github.com/snsie/ai-webdev/blob/main/images/cartpole-trained.gif?raw=true' width=\"300\"/> |"
      ],
      "metadata": {
        "id": "A-YYa0w41cqt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Fqe9TO2uVd"
      },
      "source": [
        "<center><h3><u>Actions</u></h3></center>\n",
        "\n",
        "| Num | Action                 |\n",
        "|-----|------------------------|\n",
        "| 0   | Push cart to the left  |\n",
        "| 1   | Push cart to the right |\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "<center><h3><u>States</u></h3></center>\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|:-------------------|:-----------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "<br/>\n",
        "\n",
        "[Cart Pole Python File](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL Algorithm to be used in this example:\n",
        "\n",
        "<center><h1>Q-learning</h1></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<h4>\n",
        "\\begin{align}\n",
        "Q(s,a) = r(s,a) + \\gamma \\cdot \\max_{a} Q(s',a')\n",
        "\\end{align}\n",
        "</h4>\n",
        "\n",
        "<br/>\n",
        "\n",
        "###$Q(s,a)$ = Q-value\n",
        "\n",
        "###$r(s,a)$ = reward for current action\n",
        "\n",
        "###$\\gamma$ = parameter that scales: $\\max_{a}Q(s',a')$\n",
        "\n",
        "###$\\max_{a}Q(s',a')$ = Maximum Q-value predicted in the next state \n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "u1IqACZf-7yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from: https://colab.research.google.com/github/MrSyee/pg-is-all-you-need/blob/master/02.PPO.ipynb#scrollTo=weZ2GZJDIu1u\n",
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install pyvirtualdisplay\n",
        "    from pyvirtualdisplay import Display\n",
        "    \n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(600, 400))\n",
        "    dis.start()"
      ],
      "metadata": {
        "id": "KtRw868MHSEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPWjJiOZ2uVd",
        "outputId": "c6e94f81-c751-46d4-c00f-c05b66402d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "#Create Gym\n",
        "from gym import wrappers\n",
        "envCartPole = gym.make('CartPole-v1')\n",
        "envCartPole.seed(50) #Set the seed to keep the environment consistent across runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A9R6ybk2uVg"
      },
      "source": [
        "**Experience Replay**  \n",
        "Definition: A mechanism inspired by biology that randomizes over the data removing the correlation in the observation sequence and smoothing over changes in the data distribution.  \n",
        "\n",
        "To perform an experience replay, the algorithm stores all of the agents experiences {$s_t,a_t,r_t,s_{t+1}$} at each time step in a data set. Normally in a q-learner, we would run the update rule on them. But, with experience replay we just store them.  \n",
        "\n",
        "Later during the training process these replays will be drawn uniformly from the memory queue and be ran through the update rule. There are 2 ways to handle this and I have coded both in the past. The first is to run them on every loop and the other is to run them after X amount of runs. In this code below, I run them each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EomjmDOb2uVn"
      },
      "outputs": [],
      "source": [
        "#Global Variables\n",
        "EPISODES = 500\n",
        "TRAIN_END = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yianM-dx2uVn"
      },
      "outputs": [],
      "source": [
        "#Hyper Parameters\n",
        "def discount_rate(): #Gamma\n",
        "    return 0.95\n",
        "\n",
        "def learning_rate(): #Alpha\n",
        "    return 0.001\n",
        "\n",
        "def batch_size(): #Size of the batch used in the experience replay\n",
        "    return 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsbhTZLQ2uVo"
      },
      "source": [
        "**Deep Q-Network Class**  \n",
        "The following class is the deep Q-network that is built using the neural network code from Keras.  \n",
        "**init**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This creates the class and sets the local parameters.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I use a *deque* for the local memory to hold the experiences and a keras model for the NN.  \n",
        "\n",
        "**build_model(self)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This builds the NN. I am using sequential model. Each of the layers are *Dense* despite the fact the document talks about using *Convolution*. But, they are only using that because they need to convert pixels and I already have numbers.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I am using an input layer(4), 24 neuron layer, 24 neuron layer, and an output layer(2).  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For calculating the loss I am using mean squared error.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For an optimizer I am using [Adam](https://arxiv.org/abs/1412.6980v8). It is a variant of gradient descent and you can read the technical document at the link. If you want a slightly lighter explaining you can check out [Machine Learning Mastery](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). You could also use SGD (Stochastic Gradient Descent) but Adam gives me better results and seems to be the standard in most examples.  \n",
        "\n",
        "**action(self,state):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Explore: I am using the epsilon like previous lessons.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploit: I use the NN to grab the 2 possible actions and then grab the argmax to find the better one  \n",
        "\n",
        "**test_action(self,state):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action when I am testing. I want to 100% exploit  \n",
        "\n",
        "**store(self, state, action, reward, nstate, done):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This places the observables in memory  \n",
        "\n",
        "**experience_replay(self, batch_size):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is where the training occurs. We grab the sample batches and then use the NN to predict the optimal action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CsaVrbA2uVp"
      },
      "outputs": [],
      "source": [
        "class DeepQNetwork():\n",
        "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
        "        self.nS = states\n",
        "        self.nA = actions\n",
        "        self.memory = deque([], maxlen=2500)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        #Explore/Exploit\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = self.build_model()\n",
        "        self.loss = []\n",
        "        \n",
        "    def build_model(self):\n",
        "        model = keras.Sequential() #linear stack of layers https://keras.io/models/sequential/\n",
        "        model.add(keras.layers.Dense(24, input_dim=self.nS, activation='relu')) #[Input] -> Layer 1\n",
        "        #   Dense: Densely connected layer https://keras.io/layers/core/\n",
        "        #   24: Number of neurons\n",
        "        #   input_dim: Number of input variables\n",
        "        #   activation: Rectified Linear Unit (relu) ranges >= 0\n",
        "        model.add(keras.layers.Dense(24, activation='relu')) #Layer 2 -> 3\n",
        "        model.add(keras.layers.Dense(self.nA, activation='linear')) #Layer 3 -> [output]\n",
        "        #   Size has to match the output (different actions)\n",
        "        #   Linear activation on the last layer\n",
        "        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n",
        "                      optimizer=keras.optimizers.Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n",
        "        return model\n",
        "\n",
        "    def action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.nA) #Explore\n",
        "        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def test_action(self, state): #Exploit\n",
        "        action_vals = self.model.predict(state)\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def store(self, state, action, reward, nstate, done):\n",
        "        #Store the experience in memory\n",
        "        self.memory.append( (state, action, reward, nstate, done) )\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        #Execute the experience replay\n",
        "        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n",
        "\n",
        "        #Convert to numpy for speed by vectorization\n",
        "        x = []\n",
        "        y = []\n",
        "        np_array = np.array(minibatch)\n",
        "        st = np.zeros((0,self.nS)) #States\n",
        "        nst = np.zeros( (0,self.nS) )#Next States\n",
        "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
        "            st = np.append( st, np_array[i,0], axis=0)\n",
        "            nst = np.append( nst, np_array[i,3], axis=0)\n",
        "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
        "        nst_predict = self.model.predict(nst)\n",
        "        index = 0\n",
        "        for state, action, reward, nstate, done in minibatch:\n",
        "            x.append(state)\n",
        "            #Predict from state\n",
        "            nst_action_predict_model = nst_predict[index]\n",
        "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
        "                target = reward\n",
        "            else:   #Non terminal\n",
        "                target = reward + self.gamma * np.amax(nst_action_predict_model)\n",
        "            target_f = st_predict[index]\n",
        "            target_f[action] = target\n",
        "            y.append(target_f)\n",
        "            index += 1\n",
        "        #Reshape for Keras Fit\n",
        "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
        "        y_reshape = np.array(y)\n",
        "        epoch_count = 1 #Epochs is the number or iterations\n",
        "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
        "        #Graph Losses\n",
        "        for i in range(epoch_count):\n",
        "            self.loss.append( hist.history['loss'][i] )\n",
        "        #Decay Epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azVNSMYa2uVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a2cf1d-3d26-4313-9b48-cdd7f696d993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#Create the agent\n",
        "nS = envCartPole.observation_space.shape[0] #This is only 4\n",
        "nA = envCartPole.action_space.n #Actions\n",
        "dqn = DeepQNetwork(nS, nA, learning_rate(), discount_rate(), 1, 0.001, 0.995 )\n",
        "\n",
        "batch_size = batch_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFZuVbZz2uVq",
        "outputId": "533b1867-db09-43bf-c95c-418bd0bfcd34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/500, score: 9.0, e: 1\n",
            "episode: 1/500, score: 32.0, e: 0.9229311239742362\n",
            "episode: 2/500, score: 11.0, e: 0.8778091417340573\n",
            "episode: 3/500, score: 13.0, e: 0.8265651079747222\n",
            "episode: 4/500, score: 15.0, e: 0.7705488893118823\n",
            "episode: 5/500, score: 32.0, e: 0.6596532430440636\n",
            "episode: 6/500, score: 18.0, e: 0.6057704364907278\n",
            "episode: 7/500, score: 10.0, e: 0.5790496471185967\n",
            "episode: 8/500, score: 9.0, e: 0.5562889678716474\n",
            "episode: 9/500, score: 21.0, e: 0.5032248303978422\n",
            "episode: 10/500, score: 15.0, e: 0.46912134373457726\n",
            "episode: 11/500, score: 12.0, e: 0.4439551321314536\n",
            "episode: 12/500, score: 13.0, e: 0.4180382776616619\n",
            "episode: 13/500, score: 18.0, e: 0.38389143477919885\n",
            "episode: 14/500, score: 8.0, e: 0.3706551064126331\n",
            "episode: 15/500, score: 13.0, e: 0.34901730169741024\n",
            "episode: 16/500, score: 14.0, e: 0.326999438029567\n",
            "episode: 17/500, score: 13.0, e: 0.3079101286968243\n",
            "episode: 18/500, score: 14.0, e: 0.2884855236625661\n",
            "episode: 19/500, score: 19.0, e: 0.26359640222486147\n",
            "episode: 20/500, score: 17.0, e: 0.24328132378095624\n",
            "episode: 21/500, score: 12.0, e: 0.23023039494318503\n",
            "episode: 22/500, score: 19.0, e: 0.21036724137609603\n",
            "episode: 23/500, score: 20.0, e: 0.1912566947289212\n",
            "episode: 24/500, score: 30.0, e: 0.16538114245489302\n",
            "episode: 25/500, score: 57.0, e: 0.12490462201997637\n",
            "episode: 26/500, score: 10.0, e: 0.11939502647758558\n",
            "episode: 27/500, score: 10.0, e: 0.11412846151764894\n",
            "episode: 28/500, score: 8.0, e: 0.11019338598389174\n",
            "episode: 29/500, score: 9.0, e: 0.10586201936274783\n",
            "episode: 30/500, score: 11.0, e: 0.10068643904747315\n",
            "episode: 31/500, score: 10.0, e: 0.09624511776741324\n",
            "episode: 32/500, score: 43.0, e: 0.07797369223798889\n",
            "episode: 33/500, score: 31.0, e: 0.06708733218678724\n",
            "episode: 34/500, score: 19.0, e: 0.06129936495526111\n",
            "episode: 35/500, score: 14.0, e: 0.05743227569078546\n",
            "episode: 36/500, score: 34.0, e: 0.04867631463831842\n",
            "episode: 37/500, score: 32.0, e: 0.041670929977297724\n",
            "episode: 38/500, score: 44.0, e: 0.03359121130473201\n",
            "episode: 39/500, score: 41.0, e: 0.027488364100186506\n",
            "episode: 40/500, score: 31.0, e: 0.023650553933395897\n",
            "episode: 41/500, score: 29.0, e: 0.020553584742122436\n",
            "episode: 42/500, score: 46.0, e: 0.0164031105360144\n",
            "episode: 43/500, score: 37.0, e: 0.013694844909292236\n",
            "episode: 44/500, score: 31.0, e: 0.011782828070678488\n",
            "episode: 45/500, score: 101.0, e: 0.007137688903470108\n",
            "episode: 46/500, score: 38.0, e: 0.005929411657474116\n",
            "episode: 47/500, score: 22.0, e: 0.005336988887577507\n",
            "episode: 48/500, score: 60.0, e: 0.003970617593100295\n",
            "episode: 49/500, score: 63.0, e: 0.002909973732624202\n",
            "episode: 50/500, score: 210.0, e: 0.0010207348544430564\n",
            "episode: 51/500, score: 70.0, e: 0.0009954703940636294\n",
            "episode: 52/500, score: 81.0, e: 0.0009954703940636294\n",
            "episode: 53/500, score: 43.0, e: 0.0009954703940636294\n",
            "episode: 54/500, score: 60.0, e: 0.0009954703940636294\n",
            "episode: 55/500, score: 73.0, e: 0.0009954703940636294\n",
            "episode: 56/500, score: 121.0, e: 0.0009954703940636294\n",
            "episode: 57/500, score: 78.0, e: 0.0009954703940636294\n",
            "episode: 58/500, score: 75.0, e: 0.0009954703940636294\n",
            "episode: 59/500, score: 90.0, e: 0.0009954703940636294\n",
            "episode: 60/500, score: 87.0, e: 0.0009954703940636294\n",
            "episode: 61/500, score: 97.0, e: 0.0009954703940636294\n",
            "episode: 62/500, score: 127.0, e: 0.0009954703940636294\n",
            "episode: 63/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 64/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 65/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 66/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 67/500, score: 210.0, e: 0.0009954703940636294\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "rewards = [] #Store rewards for graphing\n",
        "epsilons = [] # Store the Explore/Exploit\n",
        "TEST_Episodes = 0\n",
        "for e in range(EPISODES):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n",
        "    tot_rewards = 0\n",
        "    for time in range(210): #200 is when you \"solve\" the game. This can continue forever as far as I know\n",
        "        action = dqn.action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        nstate = np.reshape(nstate, [1, nS])\n",
        "        tot_rewards += reward\n",
        "        dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n",
        "        state = nstate\n",
        "        #done: CartPole fell. \n",
        "        #time == 209: CartPole stayed upright\n",
        "        if done or time == 209:\n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(dqn.epsilon)\n",
        "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
        "                  .format(e, EPISODES, tot_rewards, dqn.epsilon))\n",
        "            break\n",
        "        #Experience Replay\n",
        "        if len(dqn.memory) > batch_size:\n",
        "            dqn.experience_replay(batch_size)\n",
        "    #If our current NN passes we are done\n",
        "    #I am going to use the last 5 runs\n",
        "    if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n",
        "        #Set the rest of the EPISODES for testing\n",
        "        TEST_Episodes = EPISODES - e\n",
        "        TRAIN_END = e\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0hDY0PZ2uVr",
        "outputId": "55feadf5-a662-46ec-cb9c-a03f6af47a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/442, score: 210.0, e: 0\n",
            "episode: 1/442, score: 210.0, e: 0\n",
            "episode: 2/442, score: 210.0, e: 0\n",
            "episode: 3/442, score: 210.0, e: 0\n",
            "episode: 4/442, score: 210.0, e: 0\n",
            "episode: 5/442, score: 210.0, e: 0\n",
            "episode: 6/442, score: 210.0, e: 0\n",
            "episode: 7/442, score: 210.0, e: 0\n",
            "episode: 8/442, score: 210.0, e: 0\n",
            "episode: 9/442, score: 210.0, e: 0\n",
            "episode: 10/442, score: 210.0, e: 0\n",
            "episode: 11/442, score: 210.0, e: 0\n",
            "episode: 12/442, score: 210.0, e: 0\n",
            "episode: 13/442, score: 210.0, e: 0\n",
            "episode: 14/442, score: 210.0, e: 0\n",
            "episode: 15/442, score: 210.0, e: 0\n",
            "episode: 16/442, score: 210.0, e: 0\n",
            "episode: 17/442, score: 210.0, e: 0\n",
            "episode: 18/442, score: 210.0, e: 0\n",
            "episode: 19/442, score: 210.0, e: 0\n",
            "episode: 20/442, score: 210.0, e: 0\n",
            "episode: 21/442, score: 210.0, e: 0\n",
            "episode: 22/442, score: 210.0, e: 0\n",
            "episode: 23/442, score: 210.0, e: 0\n",
            "episode: 24/442, score: 210.0, e: 0\n",
            "episode: 25/442, score: 210.0, e: 0\n",
            "episode: 26/442, score: 210.0, e: 0\n",
            "episode: 27/442, score: 210.0, e: 0\n",
            "episode: 28/442, score: 210.0, e: 0\n",
            "episode: 29/442, score: 210.0, e: 0\n",
            "episode: 30/442, score: 210.0, e: 0\n",
            "episode: 31/442, score: 210.0, e: 0\n",
            "episode: 32/442, score: 210.0, e: 0\n",
            "episode: 33/442, score: 210.0, e: 0\n",
            "episode: 34/442, score: 210.0, e: 0\n",
            "episode: 35/442, score: 210.0, e: 0\n",
            "episode: 36/442, score: 210.0, e: 0\n",
            "episode: 37/442, score: 210.0, e: 0\n",
            "episode: 38/442, score: 210.0, e: 0\n",
            "episode: 39/442, score: 210.0, e: 0\n",
            "episode: 40/442, score: 210.0, e: 0\n",
            "episode: 41/442, score: 210.0, e: 0\n",
            "episode: 42/442, score: 210.0, e: 0\n",
            "episode: 43/442, score: 210.0, e: 0\n",
            "episode: 44/442, score: 210.0, e: 0\n",
            "episode: 45/442, score: 210.0, e: 0\n",
            "episode: 46/442, score: 210.0, e: 0\n",
            "episode: 47/442, score: 210.0, e: 0\n",
            "episode: 48/442, score: 210.0, e: 0\n",
            "episode: 49/442, score: 210.0, e: 0\n",
            "episode: 50/442, score: 210.0, e: 0\n",
            "episode: 51/442, score: 210.0, e: 0\n",
            "episode: 52/442, score: 210.0, e: 0\n",
            "episode: 53/442, score: 210.0, e: 0\n",
            "episode: 54/442, score: 210.0, e: 0\n",
            "episode: 55/442, score: 210.0, e: 0\n",
            "episode: 56/442, score: 210.0, e: 0\n",
            "episode: 57/442, score: 210.0, e: 0\n",
            "episode: 58/442, score: 210.0, e: 0\n",
            "episode: 59/442, score: 210.0, e: 0\n",
            "episode: 60/442, score: 210.0, e: 0\n",
            "episode: 61/442, score: 210.0, e: 0\n",
            "episode: 62/442, score: 210.0, e: 0\n",
            "episode: 63/442, score: 210.0, e: 0\n",
            "episode: 64/442, score: 210.0, e: 0\n",
            "episode: 65/442, score: 210.0, e: 0\n",
            "episode: 66/442, score: 210.0, e: 0\n",
            "episode: 67/442, score: 210.0, e: 0\n",
            "episode: 68/442, score: 210.0, e: 0\n",
            "episode: 69/442, score: 210.0, e: 0\n",
            "episode: 70/442, score: 210.0, e: 0\n",
            "episode: 71/442, score: 210.0, e: 0\n",
            "episode: 72/442, score: 210.0, e: 0\n",
            "episode: 73/442, score: 210.0, e: 0\n",
            "episode: 74/442, score: 210.0, e: 0\n",
            "episode: 75/442, score: 210.0, e: 0\n",
            "episode: 76/442, score: 210.0, e: 0\n",
            "episode: 77/442, score: 210.0, e: 0\n",
            "episode: 78/442, score: 210.0, e: 0\n",
            "episode: 79/442, score: 210.0, e: 0\n",
            "episode: 80/442, score: 210.0, e: 0\n",
            "episode: 81/442, score: 210.0, e: 0\n",
            "episode: 82/442, score: 210.0, e: 0\n",
            "episode: 83/442, score: 210.0, e: 0\n",
            "episode: 84/442, score: 210.0, e: 0\n",
            "episode: 85/442, score: 210.0, e: 0\n",
            "episode: 86/442, score: 210.0, e: 0\n",
            "episode: 87/442, score: 210.0, e: 0\n",
            "episode: 88/442, score: 210.0, e: 0\n",
            "episode: 89/442, score: 210.0, e: 0\n",
            "episode: 90/442, score: 210.0, e: 0\n",
            "episode: 91/442, score: 210.0, e: 0\n",
            "episode: 92/442, score: 210.0, e: 0\n",
            "episode: 93/442, score: 210.0, e: 0\n",
            "episode: 94/442, score: 210.0, e: 0\n",
            "episode: 95/442, score: 210.0, e: 0\n",
            "episode: 96/442, score: 210.0, e: 0\n",
            "episode: 97/442, score: 210.0, e: 0\n",
            "episode: 98/442, score: 210.0, e: 0\n",
            "episode: 99/442, score: 210.0, e: 0\n",
            "episode: 100/442, score: 210.0, e: 0\n",
            "episode: 101/442, score: 210.0, e: 0\n",
            "episode: 102/442, score: 210.0, e: 0\n",
            "episode: 103/442, score: 210.0, e: 0\n",
            "episode: 104/442, score: 210.0, e: 0\n",
            "episode: 105/442, score: 210.0, e: 0\n",
            "episode: 106/442, score: 210.0, e: 0\n",
            "episode: 107/442, score: 210.0, e: 0\n",
            "episode: 108/442, score: 210.0, e: 0\n",
            "episode: 109/442, score: 210.0, e: 0\n",
            "episode: 110/442, score: 210.0, e: 0\n",
            "episode: 111/442, score: 210.0, e: 0\n",
            "episode: 112/442, score: 210.0, e: 0\n",
            "episode: 113/442, score: 210.0, e: 0\n",
            "episode: 114/442, score: 210.0, e: 0\n",
            "episode: 115/442, score: 210.0, e: 0\n",
            "episode: 116/442, score: 210.0, e: 0\n",
            "episode: 117/442, score: 210.0, e: 0\n",
            "episode: 118/442, score: 210.0, e: 0\n",
            "episode: 119/442, score: 210.0, e: 0\n",
            "episode: 120/442, score: 210.0, e: 0\n",
            "episode: 121/442, score: 210.0, e: 0\n",
            "episode: 122/442, score: 210.0, e: 0\n",
            "episode: 123/442, score: 210.0, e: 0\n",
            "episode: 124/442, score: 210.0, e: 0\n",
            "episode: 125/442, score: 210.0, e: 0\n",
            "episode: 126/442, score: 210.0, e: 0\n",
            "episode: 127/442, score: 210.0, e: 0\n",
            "episode: 128/442, score: 210.0, e: 0\n",
            "episode: 129/442, score: 210.0, e: 0\n",
            "episode: 130/442, score: 210.0, e: 0\n",
            "episode: 131/442, score: 210.0, e: 0\n",
            "episode: 132/442, score: 210.0, e: 0\n",
            "episode: 133/442, score: 210.0, e: 0\n",
            "episode: 134/442, score: 210.0, e: 0\n",
            "episode: 135/442, score: 210.0, e: 0\n",
            "episode: 136/442, score: 210.0, e: 0\n",
            "episode: 137/442, score: 210.0, e: 0\n",
            "episode: 138/442, score: 210.0, e: 0\n",
            "episode: 139/442, score: 210.0, e: 0\n",
            "episode: 140/442, score: 210.0, e: 0\n",
            "episode: 141/442, score: 210.0, e: 0\n",
            "episode: 142/442, score: 210.0, e: 0\n",
            "episode: 143/442, score: 210.0, e: 0\n",
            "episode: 144/442, score: 210.0, e: 0\n",
            "episode: 145/442, score: 210.0, e: 0\n",
            "episode: 146/442, score: 210.0, e: 0\n",
            "episode: 147/442, score: 210.0, e: 0\n",
            "episode: 148/442, score: 210.0, e: 0\n",
            "episode: 149/442, score: 210.0, e: 0\n",
            "episode: 150/442, score: 210.0, e: 0\n",
            "episode: 151/442, score: 210.0, e: 0\n",
            "episode: 152/442, score: 210.0, e: 0\n",
            "episode: 153/442, score: 210.0, e: 0\n",
            "episode: 154/442, score: 210.0, e: 0\n",
            "episode: 155/442, score: 210.0, e: 0\n",
            "episode: 156/442, score: 210.0, e: 0\n",
            "episode: 157/442, score: 210.0, e: 0\n",
            "episode: 158/442, score: 210.0, e: 0\n",
            "episode: 159/442, score: 210.0, e: 0\n",
            "episode: 160/442, score: 210.0, e: 0\n",
            "episode: 161/442, score: 210.0, e: 0\n",
            "episode: 162/442, score: 210.0, e: 0\n",
            "episode: 163/442, score: 210.0, e: 0\n",
            "episode: 164/442, score: 210.0, e: 0\n",
            "episode: 165/442, score: 210.0, e: 0\n",
            "episode: 166/442, score: 210.0, e: 0\n",
            "episode: 167/442, score: 210.0, e: 0\n",
            "episode: 168/442, score: 210.0, e: 0\n",
            "episode: 169/442, score: 210.0, e: 0\n",
            "episode: 170/442, score: 210.0, e: 0\n",
            "episode: 171/442, score: 210.0, e: 0\n",
            "episode: 172/442, score: 210.0, e: 0\n",
            "episode: 173/442, score: 210.0, e: 0\n",
            "episode: 174/442, score: 210.0, e: 0\n",
            "episode: 175/442, score: 210.0, e: 0\n",
            "episode: 176/442, score: 210.0, e: 0\n",
            "episode: 177/442, score: 210.0, e: 0\n",
            "episode: 178/442, score: 210.0, e: 0\n",
            "episode: 179/442, score: 210.0, e: 0\n",
            "episode: 180/442, score: 210.0, e: 0\n",
            "episode: 181/442, score: 210.0, e: 0\n",
            "episode: 182/442, score: 210.0, e: 0\n",
            "episode: 183/442, score: 210.0, e: 0\n",
            "episode: 184/442, score: 210.0, e: 0\n",
            "episode: 185/442, score: 210.0, e: 0\n",
            "episode: 186/442, score: 210.0, e: 0\n",
            "episode: 187/442, score: 210.0, e: 0\n",
            "episode: 188/442, score: 210.0, e: 0\n",
            "episode: 189/442, score: 210.0, e: 0\n",
            "episode: 190/442, score: 210.0, e: 0\n",
            "episode: 191/442, score: 210.0, e: 0\n",
            "episode: 192/442, score: 210.0, e: 0\n",
            "episode: 193/442, score: 210.0, e: 0\n",
            "episode: 194/442, score: 210.0, e: 0\n",
            "episode: 195/442, score: 210.0, e: 0\n",
            "episode: 196/442, score: 210.0, e: 0\n",
            "episode: 197/442, score: 210.0, e: 0\n",
            "episode: 198/442, score: 210.0, e: 0\n",
            "episode: 199/442, score: 210.0, e: 0\n",
            "episode: 200/442, score: 210.0, e: 0\n",
            "episode: 201/442, score: 210.0, e: 0\n",
            "episode: 202/442, score: 210.0, e: 0\n",
            "episode: 203/442, score: 210.0, e: 0\n",
            "episode: 204/442, score: 210.0, e: 0\n",
            "episode: 205/442, score: 210.0, e: 0\n",
            "episode: 206/442, score: 210.0, e: 0\n",
            "episode: 207/442, score: 210.0, e: 0\n",
            "episode: 208/442, score: 210.0, e: 0\n",
            "episode: 209/442, score: 210.0, e: 0\n",
            "episode: 210/442, score: 210.0, e: 0\n",
            "episode: 211/442, score: 210.0, e: 0\n",
            "episode: 212/442, score: 210.0, e: 0\n",
            "episode: 213/442, score: 210.0, e: 0\n",
            "episode: 214/442, score: 210.0, e: 0\n",
            "episode: 215/442, score: 210.0, e: 0\n",
            "episode: 216/442, score: 210.0, e: 0\n",
            "episode: 217/442, score: 210.0, e: 0\n",
            "episode: 218/442, score: 210.0, e: 0\n",
            "episode: 219/442, score: 210.0, e: 0\n",
            "episode: 220/442, score: 210.0, e: 0\n",
            "episode: 221/442, score: 210.0, e: 0\n",
            "episode: 222/442, score: 210.0, e: 0\n",
            "episode: 223/442, score: 210.0, e: 0\n",
            "episode: 224/442, score: 210.0, e: 0\n",
            "episode: 225/442, score: 210.0, e: 0\n",
            "episode: 226/442, score: 210.0, e: 0\n",
            "episode: 227/442, score: 210.0, e: 0\n",
            "episode: 228/442, score: 210.0, e: 0\n",
            "episode: 229/442, score: 210.0, e: 0\n",
            "episode: 230/442, score: 210.0, e: 0\n",
            "episode: 231/442, score: 210.0, e: 0\n",
            "episode: 232/442, score: 210.0, e: 0\n",
            "episode: 233/442, score: 210.0, e: 0\n",
            "episode: 234/442, score: 210.0, e: 0\n",
            "episode: 235/442, score: 210.0, e: 0\n",
            "episode: 236/442, score: 210.0, e: 0\n",
            "episode: 237/442, score: 210.0, e: 0\n",
            "episode: 238/442, score: 210.0, e: 0\n",
            "episode: 239/442, score: 210.0, e: 0\n",
            "episode: 240/442, score: 210.0, e: 0\n",
            "episode: 241/442, score: 210.0, e: 0\n",
            "episode: 242/442, score: 210.0, e: 0\n",
            "episode: 243/442, score: 210.0, e: 0\n",
            "episode: 244/442, score: 210.0, e: 0\n",
            "episode: 245/442, score: 210.0, e: 0\n",
            "episode: 246/442, score: 210.0, e: 0\n",
            "episode: 247/442, score: 210.0, e: 0\n",
            "episode: 248/442, score: 210.0, e: 0\n",
            "episode: 249/442, score: 210.0, e: 0\n",
            "episode: 250/442, score: 210.0, e: 0\n",
            "episode: 251/442, score: 210.0, e: 0\n",
            "episode: 252/442, score: 210.0, e: 0\n",
            "episode: 253/442, score: 210.0, e: 0\n",
            "episode: 254/442, score: 210.0, e: 0\n",
            "episode: 255/442, score: 210.0, e: 0\n",
            "episode: 256/442, score: 210.0, e: 0\n",
            "episode: 257/442, score: 210.0, e: 0\n",
            "episode: 258/442, score: 210.0, e: 0\n",
            "episode: 259/442, score: 210.0, e: 0\n",
            "episode: 260/442, score: 210.0, e: 0\n",
            "episode: 261/442, score: 210.0, e: 0\n",
            "episode: 262/442, score: 210.0, e: 0\n",
            "episode: 263/442, score: 210.0, e: 0\n",
            "episode: 264/442, score: 210.0, e: 0\n",
            "episode: 265/442, score: 210.0, e: 0\n",
            "episode: 266/442, score: 210.0, e: 0\n",
            "episode: 267/442, score: 210.0, e: 0\n",
            "episode: 268/442, score: 210.0, e: 0\n",
            "episode: 269/442, score: 210.0, e: 0\n",
            "episode: 270/442, score: 210.0, e: 0\n",
            "episode: 271/442, score: 210.0, e: 0\n",
            "episode: 272/442, score: 210.0, e: 0\n",
            "episode: 273/442, score: 210.0, e: 0\n",
            "episode: 274/442, score: 210.0, e: 0\n",
            "episode: 275/442, score: 210.0, e: 0\n",
            "episode: 276/442, score: 210.0, e: 0\n",
            "episode: 277/442, score: 210.0, e: 0\n",
            "episode: 278/442, score: 210.0, e: 0\n",
            "episode: 279/442, score: 210.0, e: 0\n",
            "episode: 280/442, score: 210.0, e: 0\n",
            "episode: 281/442, score: 210.0, e: 0\n",
            "episode: 282/442, score: 210.0, e: 0\n",
            "episode: 283/442, score: 210.0, e: 0\n",
            "episode: 284/442, score: 210.0, e: 0\n",
            "episode: 285/442, score: 210.0, e: 0\n",
            "episode: 286/442, score: 210.0, e: 0\n",
            "episode: 287/442, score: 210.0, e: 0\n",
            "episode: 288/442, score: 210.0, e: 0\n",
            "episode: 289/442, score: 210.0, e: 0\n",
            "episode: 290/442, score: 210.0, e: 0\n",
            "episode: 291/442, score: 210.0, e: 0\n",
            "episode: 292/442, score: 210.0, e: 0\n",
            "episode: 293/442, score: 210.0, e: 0\n",
            "episode: 294/442, score: 210.0, e: 0\n",
            "episode: 295/442, score: 210.0, e: 0\n",
            "episode: 296/442, score: 210.0, e: 0\n",
            "episode: 297/442, score: 210.0, e: 0\n",
            "episode: 298/442, score: 210.0, e: 0\n",
            "episode: 299/442, score: 210.0, e: 0\n",
            "episode: 300/442, score: 210.0, e: 0\n",
            "episode: 301/442, score: 210.0, e: 0\n",
            "episode: 302/442, score: 210.0, e: 0\n",
            "episode: 303/442, score: 210.0, e: 0\n",
            "episode: 304/442, score: 210.0, e: 0\n",
            "episode: 305/442, score: 210.0, e: 0\n",
            "episode: 306/442, score: 210.0, e: 0\n",
            "episode: 307/442, score: 210.0, e: 0\n",
            "episode: 308/442, score: 210.0, e: 0\n",
            "episode: 309/442, score: 210.0, e: 0\n",
            "episode: 310/442, score: 210.0, e: 0\n",
            "episode: 311/442, score: 210.0, e: 0\n",
            "episode: 312/442, score: 210.0, e: 0\n",
            "episode: 313/442, score: 210.0, e: 0\n",
            "episode: 314/442, score: 210.0, e: 0\n",
            "episode: 315/442, score: 210.0, e: 0\n",
            "episode: 316/442, score: 210.0, e: 0\n",
            "episode: 317/442, score: 210.0, e: 0\n",
            "episode: 318/442, score: 210.0, e: 0\n",
            "episode: 319/442, score: 210.0, e: 0\n",
            "episode: 320/442, score: 210.0, e: 0\n",
            "episode: 321/442, score: 210.0, e: 0\n",
            "episode: 322/442, score: 210.0, e: 0\n",
            "episode: 323/442, score: 210.0, e: 0\n",
            "episode: 324/442, score: 210.0, e: 0\n",
            "episode: 325/442, score: 210.0, e: 0\n",
            "episode: 326/442, score: 210.0, e: 0\n",
            "episode: 327/442, score: 210.0, e: 0\n",
            "episode: 328/442, score: 210.0, e: 0\n",
            "episode: 329/442, score: 210.0, e: 0\n",
            "episode: 330/442, score: 210.0, e: 0\n",
            "episode: 331/442, score: 210.0, e: 0\n",
            "episode: 332/442, score: 210.0, e: 0\n",
            "episode: 333/442, score: 210.0, e: 0\n",
            "episode: 334/442, score: 210.0, e: 0\n",
            "episode: 335/442, score: 210.0, e: 0\n",
            "episode: 336/442, score: 210.0, e: 0\n",
            "episode: 337/442, score: 210.0, e: 0\n",
            "episode: 338/442, score: 210.0, e: 0\n",
            "episode: 339/442, score: 210.0, e: 0\n",
            "episode: 340/442, score: 210.0, e: 0\n",
            "episode: 341/442, score: 210.0, e: 0\n",
            "episode: 342/442, score: 210.0, e: 0\n",
            "episode: 343/442, score: 210.0, e: 0\n",
            "episode: 344/442, score: 210.0, e: 0\n",
            "episode: 345/442, score: 210.0, e: 0\n",
            "episode: 346/442, score: 210.0, e: 0\n",
            "episode: 347/442, score: 210.0, e: 0\n",
            "episode: 348/442, score: 210.0, e: 0\n",
            "episode: 349/442, score: 210.0, e: 0\n",
            "episode: 350/442, score: 210.0, e: 0\n",
            "episode: 351/442, score: 210.0, e: 0\n",
            "episode: 352/442, score: 210.0, e: 0\n",
            "episode: 353/442, score: 210.0, e: 0\n",
            "episode: 354/442, score: 210.0, e: 0\n",
            "episode: 355/442, score: 210.0, e: 0\n",
            "episode: 356/442, score: 210.0, e: 0\n",
            "episode: 357/442, score: 210.0, e: 0\n",
            "episode: 358/442, score: 210.0, e: 0\n",
            "episode: 359/442, score: 210.0, e: 0\n",
            "episode: 360/442, score: 210.0, e: 0\n",
            "episode: 361/442, score: 210.0, e: 0\n",
            "episode: 362/442, score: 210.0, e: 0\n",
            "episode: 363/442, score: 210.0, e: 0\n",
            "episode: 364/442, score: 210.0, e: 0\n",
            "episode: 365/442, score: 210.0, e: 0\n",
            "episode: 366/442, score: 210.0, e: 0\n",
            "episode: 367/442, score: 210.0, e: 0\n",
            "episode: 368/442, score: 210.0, e: 0\n",
            "episode: 369/442, score: 210.0, e: 0\n",
            "episode: 370/442, score: 210.0, e: 0\n",
            "episode: 371/442, score: 210.0, e: 0\n",
            "episode: 372/442, score: 210.0, e: 0\n",
            "episode: 373/442, score: 210.0, e: 0\n",
            "episode: 374/442, score: 210.0, e: 0\n",
            "episode: 375/442, score: 210.0, e: 0\n",
            "episode: 376/442, score: 210.0, e: 0\n",
            "episode: 377/442, score: 210.0, e: 0\n",
            "episode: 378/442, score: 210.0, e: 0\n",
            "episode: 379/442, score: 210.0, e: 0\n",
            "episode: 380/442, score: 210.0, e: 0\n",
            "episode: 381/442, score: 210.0, e: 0\n",
            "episode: 382/442, score: 210.0, e: 0\n",
            "episode: 383/442, score: 210.0, e: 0\n",
            "episode: 384/442, score: 210.0, e: 0\n",
            "episode: 385/442, score: 210.0, e: 0\n",
            "episode: 386/442, score: 210.0, e: 0\n",
            "episode: 387/442, score: 210.0, e: 0\n",
            "episode: 388/442, score: 210.0, e: 0\n",
            "episode: 389/442, score: 210.0, e: 0\n",
            "episode: 390/442, score: 210.0, e: 0\n",
            "episode: 391/442, score: 210.0, e: 0\n",
            "episode: 392/442, score: 210.0, e: 0\n",
            "episode: 393/442, score: 210.0, e: 0\n",
            "episode: 394/442, score: 210.0, e: 0\n",
            "episode: 395/442, score: 210.0, e: 0\n",
            "episode: 396/442, score: 210.0, e: 0\n",
            "episode: 397/442, score: 210.0, e: 0\n",
            "episode: 398/442, score: 210.0, e: 0\n",
            "episode: 399/442, score: 210.0, e: 0\n",
            "episode: 400/442, score: 210.0, e: 0\n",
            "episode: 401/442, score: 210.0, e: 0\n",
            "episode: 402/442, score: 210.0, e: 0\n",
            "episode: 403/442, score: 210.0, e: 0\n",
            "episode: 404/442, score: 210.0, e: 0\n",
            "episode: 405/442, score: 210.0, e: 0\n",
            "episode: 406/442, score: 210.0, e: 0\n",
            "episode: 407/442, score: 210.0, e: 0\n",
            "episode: 408/442, score: 210.0, e: 0\n",
            "episode: 409/442, score: 210.0, e: 0\n",
            "episode: 410/442, score: 210.0, e: 0\n",
            "episode: 411/442, score: 210.0, e: 0\n",
            "episode: 412/442, score: 210.0, e: 0\n",
            "episode: 413/442, score: 210.0, e: 0\n",
            "episode: 414/442, score: 210.0, e: 0\n",
            "episode: 415/442, score: 210.0, e: 0\n",
            "episode: 416/442, score: 210.0, e: 0\n",
            "episode: 417/442, score: 210.0, e: 0\n",
            "episode: 418/442, score: 210.0, e: 0\n",
            "episode: 419/442, score: 210.0, e: 0\n",
            "episode: 420/442, score: 210.0, e: 0\n",
            "episode: 421/442, score: 210.0, e: 0\n",
            "episode: 422/442, score: 210.0, e: 0\n",
            "episode: 423/442, score: 210.0, e: 0\n",
            "episode: 424/442, score: 210.0, e: 0\n",
            "episode: 425/442, score: 210.0, e: 0\n",
            "episode: 426/442, score: 210.0, e: 0\n",
            "episode: 427/442, score: 210.0, e: 0\n",
            "episode: 428/442, score: 210.0, e: 0\n",
            "episode: 429/442, score: 210.0, e: 0\n",
            "episode: 430/442, score: 210.0, e: 0\n",
            "episode: 431/442, score: 210.0, e: 0\n",
            "episode: 432/442, score: 210.0, e: 0\n",
            "episode: 433/442, score: 210.0, e: 0\n",
            "episode: 434/442, score: 210.0, e: 0\n",
            "episode: 435/442, score: 210.0, e: 0\n",
            "episode: 436/442, score: 210.0, e: 0\n",
            "episode: 437/442, score: 210.0, e: 0\n",
            "episode: 438/442, score: 210.0, e: 0\n",
            "episode: 439/442, score: 210.0, e: 0\n",
            "episode: 440/442, score: 210.0, e: 0\n",
            "episode: 441/442, score: 210.0, e: 0\n"
          ]
        }
      ],
      "source": [
        "#Test the agent that was trained\n",
        "#   In this section we ALWAYS use exploit don't train any more\n",
        "for e_test in range(TEST_Episodes):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS])\n",
        "    tot_rewards = 0\n",
        "    for t_test in range(210):\n",
        "        action = dqn.test_action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        nstate = np.reshape( nstate, [1, nS])\n",
        "        tot_rewards += reward\n",
        "        #DON'T STORE ANYTHING DURING TESTING\n",
        "        state = nstate\n",
        "        #done: CartPole fell. \n",
        "        #t_test == 209: CartPole stayed upright\n",
        "        if done or t_test == 209: \n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(0) #We are doing full exploit\n",
        "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
        "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
        "            break;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJyZYjHf2uVr"
      },
      "source": [
        "**Results**  \n",
        "Here is a graph of the results. If everything was done correctly you should see the rewards over the red line.  \n",
        "\n",
        "Black: This is the 100 episode rolling average  \n",
        "Red: This is the \"solved\" line at 195  \n",
        "Blue: This is the reward for each episode  \n",
        "Green: This is the value of epsilon scaled by 200  \n",
        "Yellow: This is where the tests started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C265YboP2uVs",
        "outputId": "9ce9280f-080f-4962-e214-f18b832dec0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.plot(rolling_average, color='black')\n",
        "plt.axhline(y=195, color='r', linestyle='-') #Solved Line\n",
        "#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n",
        "eps_graph = [200*x for x in epsilons]\n",
        "plt.plot(eps_graph, color='g', linestyle='-')\n",
        "#Plot the line where TESTING begins\n",
        "plt.axvline(x=TRAIN_END, color='y', linestyle='-')\n",
        "plt.xlim( (0,EPISODES) )\n",
        "plt.ylim( (0,220) )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "envCartPole.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # test\n",
        "# if IN_COLAB:\n",
        "#     agent.env = gym.wrappers.Monitor(agent.env, \"videos\", force=True)\n",
        "# frames = agent.test()"
      ],
      "metadata": {
        "id": "KgtBDepYHtTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from: https://colab.research.google.com/github/MrSyee/pg-is-all-you-need/blob/master/02.PPO.ipynb#scrollTo=_9SokMoDIu18\n",
        "if IN_COLAB:  # for colab\n",
        "    import base64\n",
        "    import glob\n",
        "    import io\n",
        "    import os\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    def ipython_show_video(path: str) -> None:\n",
        "        \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "        if not os.path.isfile(path):\n",
        "            raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "        video = io.open(path, \"r+b\").read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        display(HTML(\n",
        "            data=\"\"\"\n",
        "            <video alt=\"test\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "            </video>\n",
        "            \"\"\".format(encoded.decode(\"ascii\"))\n",
        "        ))\n",
        "\n",
        "    list_of_files = glob.glob(\"videos/*.mp4\")\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(latest_file)\n",
        "    ipython_show_video(latest_file)\n",
        "\n",
        "else:  # for jupyter\n",
        "    from matplotlib import animation\n",
        "    from JSAnimation.IPython_display import display_animation\n",
        "    from IPython.display import display\n",
        "\n",
        "\n",
        "    def display_frames_as_gif(frames):\n",
        "        \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "        patch = plt.imshow(frames[0])\n",
        "        plt.axis('off')\n",
        "\n",
        "        def animate(i):\n",
        "            patch.set_data(frames[i])\n",
        "\n",
        "        anim = animation.FuncAnimation(\n",
        "            plt.gcf(), animate, frames = len(frames), interval=50\n",
        "        )\n",
        "        display(display_animation(anim, default_mode='loop'))\n",
        "\n",
        "\n",
        "    # display \n",
        "    display_frames_as_gif(frames)"
      ],
      "metadata": {
        "id": "oWEs0GSxG8Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmbHdtuP2uVs"
      },
      "source": [
        "**Changes**  \n",
        "*hyper parameters*: You can alter alpha, gamma, batch size, and episode length to see what differences the algorithm returns.  \n",
        "*Training End*: You can also change the line where I only check the last 5 runs before switching to testing mode (if len(rewards) > 5 and np.average(rewards[-5:]) > 195:) as that doesn't prove it was solved. The reason I did this was because I wanted to limit the amount of runs I made.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yExWip-F2uVs"
      },
      "source": [
        "**Conclusion**  \n",
        "This is a Deep Q-Network implementation. There are some changes you can make here and there but it follows the paper. Hopefully, you were able to understand the code as well as make your own version to compare with this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "YNoagP352uVt"
      },
      "source": [
        "**Reference**  \n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:gdg_denver] *",
      "language": "python",
      "name": "conda-env-gdg_denver-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "lesson-04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}